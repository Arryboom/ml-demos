{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with ULMFit on Amazon Reviews\n",
    "In this notebook, we show how transfer learning can be applied to detecting the sentiment of amazon reviews, between positive and negative reviews.\n",
    "\n",
    "This notebook uses the work from Howard and Ruder, Ulmfit.\n",
    "The idea of the paper (and it implementation explained in the fast.ai deep learning course) is to learn a language model trained on a very large dataset, e.g. a Wikipedia dump. The intuition is that if a model is able to predict the next word at each word, it means it has learnt something about the structure of the language we are using.\n",
    "\n",
    "Word2vec and the likes have lead to huge improvements on various NLP tasks. This could be seen as a first step to transfer learning, where the pre-trained word vectors correspond to a transfer of the embedding layer.\n",
    "The ambition of Ulmfit (and others like ELMO or the Transformer language model recently introduced) is to progressively move the NLP field to the state where Computer Vision has risen thanks to the ImageNet challenge. Thanks to the ImageNet chalenge, today it is easy to download a model pre-trained on massive dataset of images, remove the last layer and replace it by a classifier or a regressor depending on the interest. With Ulmfit, the ambition is to be able to use a pre-trained language model and use it a backbone which we can use along with a classifier and a regressor. The game-changing apect of transfer learning is that we are no longer limited by the size of trzining data! With only a fraction of the data size that was necessary before, we can trtain a classifier/regressor and have very good result with few labelled data.\n",
    "\n",
    "Given that labelled text data are difficult to get, in comparison with unlabelled text data which is almost infinite, transfer learning is likely to change radically the field of NLP, and help lead to a maturity state closer to computyer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrate the power of Ulmfit on a dataset of Amazon reviews available on Kaggle at https://www.kaggle.com/bittlingmayer/amazonreviews/home.\n",
    "We use code from the excellent fastai course and use it for a different dataset. The original code is available at https://github.com/fastai/fastai/tree/master/courses/dl2\n",
    "\n",
    "The train set consists of 400k reviews that are either positive or negative. Training a model with FastText classifier results in a f1 score of 0.916.\n",
    "We show that uing only a fraction of this dataset we are able to reach similar and even better results.\n",
    "\n",
    "We encourage you to try on a custom classification task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is organized as such:\n",
    "\n",
    "- Tokenize the reviews and create dictionaries\n",
    "- Download a pre-trained model and link the dictionary to the embedding layer of the model\n",
    "- Fine-tune the language model on the amaxon reviews texts\n",
    "\n",
    "We have then the backbone of our algorithm: a pre-trained language model fine-tuned on Amazon reviews\n",
    "\n",
    "- Add a classifier to the language model and train the classifier layer only\n",
    "- Gradually defreeze successive layers to train different layers on the amazon reviews\n",
    "- Run a full classification task for several epochs\n",
    "- Use the model for inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, you should download the data from https://www.kaggle.com/bittlingmayer/amazonreviews, and put the extracted files into an ./Amazon folder somewher you like, and use this path for this notebook.\n",
    "\n",
    "Also, we recommend working on a dedicated environment (e.g. mkvirtualenv fastai). Then clone the fastai github repo https://github.com/fastai/fastai and install requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, \\\n",
    "confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/your/path/to/folder/Amazon'\n",
    "train = []\n",
    "with open(os.path.join(path, 'train.ft.txt'), 'r') as file:\n",
    "    for line in file:\n",
    "        train.append(file.readline())\n",
    "        \n",
    "test = []\n",
    "with open(os.path.join(path, 'test.ft.txt'), 'r') as file:\n",
    "    for line in file:\n",
    "        test.append(file.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The train data contains {len(train)} examples')\n",
    "print(f'The test data contains {len(test)} examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "PATH=Path('/your/path/to/folder/Amazon')\n",
    "\n",
    "CLAS_PATH=PATH/'amazon_class'\n",
    "CLAS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "LM_PATH=PATH/'amazon_lm'\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each item is '__label__1/2' and then the review so we split to get texts and labels\n",
    "trn_texts,trn_labels = [text[10:] for text in train], [text[:10] for text in train]\n",
    "trn_labels = [0 if label == '__label__1' else 1 for label in trn_labels]\n",
    "val_texts,val_labels = [text[10:] for text in test], [text[:10] for text in test]\n",
    "val_labels = [0 if label == '__label__1' else 1 for label in val_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following fast.ai recommendations we put our data in pandas dataframes\n",
    "col_names = ['labels','text']\n",
    "\n",
    "df_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text':val_texts, 'labels':val_labels}, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn.to_csv(CLAS_PATH/'train.csv', header=False, index=False)\n",
    "df_val.to_csv(CLAS_PATH/'test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['neg', 'pos']\n",
    "(CLAS_PATH/'classes.txt').open('w').writelines(f'{o}\\n' for o in CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to fine tune the language model so it's ok to take some of the test set in our train data\n",
    "# for the lm fine-tuning\n",
    "trn_texts,val_texts = train_test_split(np.concatenate([trn_texts,val_texts]), test_size=0.1)\n",
    "\n",
    "df_trn = pd.DataFrame({'text':trn_texts, 'labels':[0]*len(trn_texts)}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text':val_texts, 'labels':[0]*len(val_texts)}, columns=col_names)\n",
    "\n",
    "df_trn.to_csv(LM_PATH/'train.csv', header=False, index=False)\n",
    "df_val.to_csv(LM_PATH/'test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use functions from the fast.ai course to get data\n",
    "\n",
    "chunksize=24000\n",
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "def get_texts(df, n_lbls=1):\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls+1, len(df.columns)): \n",
    "        texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    return tok, list(labels)\n",
    "\n",
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = get_texts(r, n_lbls)\n",
    "        tok += tok_;\n",
    "        labels += labels_\n",
    "    return tok, labels\n",
    "\n",
    "df_trn = pd.read_csv(LM_PATH/'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(LM_PATH/'test.csv', header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell can take quite some time if your dataset is large\n",
    "# Run it once and comment it for later use\n",
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell once and comment everything but the load statements for later use\n",
    "\n",
    "(LM_PATH/'tmp').mkdir(exist_ok=True)\n",
    "np.save(LM_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(LM_PATH/'tmp'/'tok_val.npy', tok_val)\n",
    "tok_trn = np.load(LM_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(LM_PATH/'tmp'/'tok_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the most common tokens\n",
    "freq = Counter(p for o in tok_trn for p in o)\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the least common tokens\n",
    "freq.most_common()[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your vocabulary by keeping only the most common tokens that appears frequently enough\n",
    "# and constrain the size of your vocabulary. We follow here the 60k recommendation.\n",
    "max_vocab = 60000\n",
    "min_freq = 2\n",
    "\n",
    "itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)\n",
    "\n",
    "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)\n",
    "np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)\n",
    "pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything\n",
    "trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')\n",
    "val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy')\n",
    "itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs=len(itos)\n",
    "vs,len(trn_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre trained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell to download the pre-trained model.\n",
    "# It will be placed into the PATH that you defined earlier.\n",
    "# ! wget -nH -r -np -P {PATH} http://files.fast.ai/models/wt103/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights of the model\n",
    "em_sz,nh,nl = 400,1150,3\n",
    "\n",
    "PRE_PATH = PATH/'models'/'wt103'\n",
    "PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'\n",
    "\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the word embedding layer and keep a 'mean word' for unknown tokens\n",
    "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)\n",
    "\n",
    "enc_wgts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary on which the pre-trained model was trained\n",
    "# Define an embedding matrix with the vocabulary of our dataset\n",
    "itos2 = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb'))\n",
    "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})\n",
    "\n",
    "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "for i,w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r>=0 else row_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new embedding matrix for the pre-trained model\n",
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learner object to do the fine-tuning\n",
    "# Here we will freeze everything except the embedding layer, so that we can have a better \n",
    "# embedding for unknown words than just the mean embedding on which we initialise it.\n",
    "wd=1e-7\n",
    "bptt=70\n",
    "bs=52\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "\n",
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)\n",
    "\n",
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
    "\n",
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "learner.metrics = [accuracy]\n",
    "learner.freeze_to(-1)\n",
    "\n",
    "learner.model.load_state_dict(wgts)\n",
    "\n",
    "lr=1e-3\n",
    "lrs = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one epoch of fine-tuning \n",
    "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and unfreeze everything to later fine-tune the whole model\n",
    "learner.save('lm_last_ft')\n",
    "learner.load('lm_last_ft')\n",
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you want to highly tune the LM to the Amazon data, with 15 epochs\n",
    "# use_clr controls the shape of the cyclical (triangular) learning rate\n",
    "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Backbone for further classification!!\n",
    "learner.save('lm1')\n",
    "learner.save_encoder('lm1_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going back to classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we spent some time fine-tuning the language model on our Amazon data, let's see if we can classify easily these reviews.\n",
    "As before, some cells should be run once, and then use data loaders for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(CLAS_PATH/'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(CLAS_PATH/'test.csv', header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CLAS_PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels)\n",
    "np.save(CLAS_PATH/'tmp'/'val_labels.npy', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_clas = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'trn_ids.npy', trn_clas)\n",
    "np.save(CLAS_PATH/'tmp'/'val_ids.npy', val_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select here the 'size' first reviews of our dataset\n",
    "# The paper claims that it's possible to achieve very good results with few labeled examples\n",
    "# So let's try with 100 examples for training, and 1000 examples for validation.\n",
    "# We encourage you to try different values to see the effect of data size on performance.\n",
    "size = 100\n",
    "trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')[:size]\n",
    "val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')[:10*size]\n",
    "\n",
    "trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))[:size]\n",
    "val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))[:10*size]\n",
    "\n",
    "bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "vs = len(itos)\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "bs = 48\n",
    "\n",
    "min_lbl = trn_labels.min()\n",
    "trn_labels -= min_lbl\n",
    "val_labels -= min_lbl\n",
    "c=int(trn_labels.max())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trn_clas), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ccheck that the validation dataset is well balanced so acccuracy is a good metric\n",
    "# We'll also check other metrics usual for binary classification (precision, recall, f1 score)\n",
    "len(trn_labels[trn_labels == 1]) / len(trn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "val_ds = TextDataset(val_clas, val_labels)\n",
    "trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the model, here it a classifier on top of an RNN language model\n",
    "# We load the language model encoder that we fine tuned before\n",
    "# We freeze everything but the last layer, so that we can train the classification layer only.\n",
    "load the saved weights from before, and freeze everything until the last layer\n",
    "\n",
    "md = ModelData(PATH, trn_dl, val_dl)\n",
    "dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])\n",
    "\n",
    "m = get_rnn_classifier(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "          layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "          dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "\n",
    "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learn.clip=25.\n",
    "learn.metrics = [accuracy]\n",
    "\n",
    "lr=3e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "\n",
    "lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "\n",
    "wd = 1e-7\n",
    "wd = 0\n",
    "learn.load_encoder('lm1_enc')\n",
    "\n",
    "learn.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(lrs/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one epoch on the classification layer\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "learn.save('clas_0')\n",
    "learn.load('clas_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradually unfreeze another layer to train a bit more parameters than just the classifier layer\n",
    "learn.freeze_to(-2)\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "learn.save('clas_1')\n",
    "learn.load('clas_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze everything and train for a few epochs on the whole set of parameters of the model\n",
    "learn.unfreeze()\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "learn.save('clas_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "Nonw, let's play with the model we've just learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from the fast.ai imdb scripts \n",
    "def load_model(itos_filename, classifier_filename):\n",
    "    \"\"\"Load the classifier and int to string mapping\n",
    "    Args:\n",
    "        itos_filename (str): The filename of the int to string mapping file (usually called itos.pkl)\n",
    "        classifier_filename (str): The filename of the trained classifier\n",
    "    Returns:\n",
    "        string to int mapping, trained classifer model\n",
    "    \"\"\"\n",
    "\n",
    "    # load the int to string mapping file\n",
    "    itos = pickle.load(Path(itos_filename).open('rb'))\n",
    "    # turn it into a string to int mapping (which is what we need)\n",
    "    stoi = collections.defaultdict(lambda:0, {str(v):int(k) for k,v in enumerate(itos)})\n",
    "\n",
    "    # these parameters aren't used, but this is the easiest way to get a model\n",
    "    bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "    dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5\n",
    "    num_classes = 2 # this is the number of classes we want to predict\n",
    "    vs = len(itos)\n",
    "\n",
    "    model = get_rnn_classifer(bptt, 20*70, num_classes, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "            layers=[em_sz*3, 50, num_classes], drops=[dps[4], 0.1],\n",
    "            dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "    # load the trained classifier\n",
    "    model.load_state_dict(torch.load(classifier_filename, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    # put the classifier into evaluation mode\n",
    "    model.reset()\n",
    "    model.eval()\n",
    "\n",
    "    return stoi, model\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    '''\n",
    "    Numpy Softmax, via comments on https://gist.github.com/stober/1946926\n",
    "    >>> res = softmax(np.array([0, 200, 10]))\n",
    "    >>> np.sum(res)\n",
    "    1.0\n",
    "    >>> np.all(np.abs(res - np.array([0, 1, 0])) < 0.0001)\n",
    "    True\n",
    "    >>> res = softmax(np.array([[0, 200, 10], [0, 10, 200], [200, 0, 10]]))\n",
    "    >>> np.sum(res, axis=1)\n",
    "    array([ 1.,  1.,  1.])\n",
    "    >>> res = softmax(np.array([[0, 200, 10], [0, 10, 200]]))\n",
    "    >>> np.sum(res, axis=1)\n",
    "    array([ 1.,  1.])\n",
    "    '''\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape((1, -1))\n",
    "    max_x = np.max(x, axis=1).reshape((-1, 1))\n",
    "    exp_x = np.exp(x - max_x)\n",
    "    return exp_x / np.sum(exp_x, axis=1).reshape((-1, 1))\n",
    "\n",
    "\n",
    "def predict_text(stoi, model, text):\n",
    "    \"\"\"Do the actual prediction on the text using the\n",
    "        model and mapping files passed\n",
    "    \"\"\"\n",
    "\n",
    "    # prefix text with tokens:\n",
    "    #   xbos: beginning of sentence\n",
    "    #   xfld 1: we are using a single field here\n",
    "    \n",
    "    input_str = 'xbos xfld 1 ' + text\n",
    "\n",
    "    # predictions are done on arrays of input.\n",
    "    # We only have a single input, so turn it into a 1x1 array\n",
    "    texts = [input_str]\n",
    "\n",
    "    # tokenize using the fastai wrapper around spacy\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "\n",
    "    # turn into integers for each word\n",
    "    encoded = [stoi[p] for p in tok[0]]\n",
    "\n",
    "    # we want a [x,1] array where x is the number\n",
    "    #  of words inputted (including the prefix tokens)\n",
    "    ary = np.reshape(np.array(encoded),(-1,1))\n",
    "\n",
    "    # turn this array into a tensor\n",
    "    tensor = torch.from_numpy(ary)\n",
    "\n",
    "    # wrap in a torch Variable\n",
    "    variable = Variable(tensor)\n",
    "    \n",
    "    start = time()\n",
    "\n",
    "    # do the predictions\n",
    "    predictions = model(variable)\n",
    "\n",
    "    # convert back to numpy\n",
    "    numpy_preds = predictions[0].data.numpy()\n",
    "\n",
    "    return softmax(numpy_preds[0])[0]\n",
    "\n",
    "\n",
    "def predict_input(itos_filename, trained_classifier_filename):\n",
    "\n",
    "    # Check the itos file exists\n",
    "    if not os.path.exists(itos_filename):\n",
    "        print(\"Could not find \" + itos_filename)\n",
    "        exit(-1)\n",
    "\n",
    "    # Check the classifier file exists\n",
    "    if not os.path.exists(trained_classifier_filename):\n",
    "        print(\"Could not find \" + trained_classifier_filename)\n",
    "        exit(-1)\n",
    "\n",
    "    stoi, model = load_model(itos_filename, trained_classifier_filename)\n",
    "\n",
    "    while True:\n",
    "        text = input(\"Enter text to analyse (or q to quit): \")\n",
    "        if text == 'q':\n",
    "            break\n",
    "        else:\n",
    "            scores = predict_text(stoi, model, text)\n",
    "            classes = [False, True]\n",
    "            print(\"Result: {0}, Scores: {1}\".format(classes[np.argmax(scores)], scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model and vocabulary\n",
    "itos_filename = LM_PATH/'tmp'/'itos.pkl'\n",
    "trained_classifier_filename = PATH/'models'/'clas_2.h5'\n",
    "stoi1, model1 = load_model(itos_filename, trained_classifier_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check that this positive review is predicted as positive\n",
    "text = \"I love the Feedly app!\"\n",
    "predict_text(stoi=stoi1, model=model1, text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check that this negative review is predicted as negative\n",
    "text = \"I don't love the Feedly app!\"\n",
    "predict_text(stoi=stoi1, model=model1, text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(stoi, model, texts):\n",
    "    \"\"\"Do the prediction on a list of texts\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        text = np.reshape(np.array(text),(-1,1))\n",
    "        # turn this array into a tensor\n",
    "        tensor = torch.from_numpy(text)\n",
    "        # wrap in a torch Variable\n",
    "        variable = Variable(tensor)\n",
    "        # do the predictions\n",
    "        predictions = model(variable)\n",
    "        # convert back to numpy\n",
    "        numpy_preds = predictions[0].data.numpy()\n",
    "        y.append(np.argmax(softmax(numpy_preds[0])[0]))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = prediction(stoi1, model1, val_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show relevant metrics for binary classification\n",
    "# We encourage you to try training the classifier with different data size and its effect on performance\n",
    "print(f'Accuracy --> {accuracy_score(y2, val_labels)}')\n",
    "print(f'Precision --> {precision_score(y2, val_labels)}')\n",
    "print(f'F1 score --> {f1_score(y2, val_labels)}')\n",
    "print(f'Recall score --> {recall_score(y2, val_labels)}')\n",
    "print(confusion_matrix(y2, val_labels))\n",
    "print(classification_report(y2, val_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
